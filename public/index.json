[
{
	"uri": "http://localhost:1313/AWS-fcj/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/AWS-fcj/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/AWS-fcj/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/AWS-fcj/3-blogstranslated/3.4-blog4/",
	"title": "Blog 4",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/AWS-fcj/3-blogstranslated/3.5-blog5/",
	"title": "Blog 5",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/AWS-fcj/3-blogstranslated/3.6-blog6/",
	"title": "Blog 6",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/AWS-fcj/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nSummary Report: “GenAI-powered App-DB Modernization workshop” Event Objectives Share best practices in modern application design Introduce Domain-Driven Design (DDD) and event-driven architecture Provide guidance on selecting the right compute services Present AI tools to support the development lifecycle Speakers Jignesh Shah – Director, Open Source Databases Erica Liu – Sr. GTM Specialist, AppMod Fabrianne Effendi – Assc. Specialist SA, Serverless Amazon Web Services Key Highlights Identifying the drawbacks of legacy application architecture Long product release cycles → Lost revenue/missed opportunities Inefficient operations → Reduced productivity, higher costs Non-compliance with security regulations → Security breaches, loss of reputation Transitioning to modern application architecture – Microservices Migrating to a modular system — each function is an independent service communicating via events, built on three core pillars:\nQueue Management: Handle asynchronous tasks Caching Strategy: Optimize performance Message Handling: Flexible inter-service communication Domain-Driven Design (DDD) Four-step method: Identify domain events → arrange timeline → identify actors → define bounded contexts Bookstore case study: Demonstrates real-world DDD application Context mapping: 7 patterns for integrating bounded contexts Event-Driven Architecture 3 integration patterns: Publish/Subscribe, Point-to-point, Streaming Benefits: Loose coupling, scalability, resilience Sync vs async comparison: Understanding the trade-offs Compute Evolution Shared Responsibility Model: EC2 → ECS → Fargate → Lambda Serverless benefits: No server management, auto-scaling, pay-for-value Functions vs Containers: Criteria for appropriate choice Amazon Q Developer SDLC automation: From planning to maintenance Code transformation: Java upgrade, .NET modernization AWS Transform agents: VMware, Mainframe, .NET migration Key Takeaways Design Mindset Business-first approach: Always start from the business domain, not the technology Ubiquitous language: Importance of a shared vocabulary between business and tech teams Bounded contexts: Identifying and managing complexity in large systems Technical Architecture Event storming technique: Practical method for modeling business processes Use event-driven communication instead of synchronous calls Integration patterns: When to use sync, async, pub/sub, streaming Compute spectrum: Criteria for choosing between VM, containers, and serverless Modernization Strategy Phased approach: No rushing — follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Applying to Work Apply DDD to current projects: Event storming sessions with business teams Refactor microservices: Use bounded contexts to define service boundaries Implement event-driven patterns: Replace some sync calls with async messaging Adopt serverless: Pilot AWS Lambda for suitable use cases Try Amazon Q Developer: Integrate into the dev workflow to boost productivity Event Experience Attending the “GenAI-powered App-DB Modernization” workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. Some event photos Add your event photos here\nOverall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"
},
{
	"uri": "http://localhost:1313/AWS-fcj/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nSummary Report: “GenAI-powered App-DB Modernization workshop” Event Objectives Share best practices in modern application design Introduce Domain-Driven Design (DDD) and event-driven architecture Provide guidance on selecting the right compute services Present AI tools to support the development lifecycle Speakers Jignesh Shah – Director, Open Source Databases Erica Liu – Sr. GTM Specialist, AppMod Fabrianne Effendi – Assc. Specialist SA, Serverless Amazon Web Services Key Highlights Identifying the drawbacks of legacy application architecture Long product release cycles → Lost revenue/missed opportunities Inefficient operations → Reduced productivity, higher costs Non-compliance with security regulations → Security breaches, loss of reputation Transitioning to modern application architecture – Microservices Migrating to a modular system — each function is an independent service communicating via events, built on three core pillars:\nQueue Management: Handle asynchronous tasks Caching Strategy: Optimize performance Message Handling: Flexible inter-service communication Domain-Driven Design (DDD) Four-step method: Identify domain events → arrange timeline → identify actors → define bounded contexts Bookstore case study: Demonstrates real-world DDD application Context mapping: 7 patterns for integrating bounded contexts Event-Driven Architecture 3 integration patterns: Publish/Subscribe, Point-to-point, Streaming Benefits: Loose coupling, scalability, resilience Sync vs async comparison: Understanding the trade-offs Compute Evolution Shared Responsibility Model: EC2 → ECS → Fargate → Lambda Serverless benefits: No server management, auto-scaling, pay-for-value Functions vs Containers: Criteria for appropriate choice Amazon Q Developer SDLC automation: From planning to maintenance Code transformation: Java upgrade, .NET modernization AWS Transform agents: VMware, Mainframe, .NET migration Key Takeaways Design Mindset Business-first approach: Always start from the business domain, not the technology Ubiquitous language: Importance of a shared vocabulary between business and tech teams Bounded contexts: Identifying and managing complexity in large systems Technical Architecture Event storming technique: Practical method for modeling business processes Use event-driven communication instead of synchronous calls Integration patterns: When to use sync, async, pub/sub, streaming Compute spectrum: Criteria for choosing between VM, containers, and serverless Modernization Strategy Phased approach: No rushing — follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Applying to Work Apply DDD to current projects: Event storming sessions with business teams Refactor microservices: Use bounded contexts to define service boundaries Implement event-driven patterns: Replace some sync calls with async messaging Adopt serverless: Pilot AWS Lambda for suitable use cases Try Amazon Q Developer: Integrate into the dev workflow to boost productivity Event Experience Attending the “GenAI-powered App-DB Modernization” workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. Some event photos Add your event photos here\nOverall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"
},
{
	"uri": "http://localhost:1313/AWS-fcj/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nStudent Information: Full Name: Nguyen Dang Khoi\nPhone Number: 0939093231\nEmail: dangkhoi88x@gmail.com\nUniversity: Saigon University\nMajor: Information Technology\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 29/09/2025 to 22/11/2025\nStudent Information: Full Name: Lam Vinh Cuong\nPhone Number: 0903563176\nEmail: cuonglvse182626@fpt.edu.vn\nUniversity: FPT University\nMajor: Information Technology\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 29/09/2025 to 22/11/2025\nStudent Information: Full Name: Duong Tuan Kiet\nPhone Number: 0793812792\nEmail: kietdtse183938@fpt.edu.vn\nUniversity: FPT University\nMajor: Information Technology\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 29/09/2025 to 22/11/2025\nStudent Information: Full Name: Ho Chi Kiet\nPhone Number: 0818227828\nEmail: -kiethcse182293@fpt.edu.vn\nUniversity: FPT University\nMajor: Information Technology\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 29/09/2025 to 22/11/2025 Student Information: Full Name: Nguyen Hoang Gia Huy\nPhone Number: 0902566797\nEmail: -huynhgse182631@fpt.edu.vn\nUniversity: FPT University\nMajor: Information Technology\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 29/09/2025 to 22/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "http://localhost:1313/AWS-fcj/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Time Period: 29/09/2025 - 05/10/2025\nTasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 29/09/2025 29/09/2025 2 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 30/09/2025 30/09/2025 https://cloudjourney.awsstudygroup.com/ 3 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 01/10/2025 01/10/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 02/10/2025 03/10/2025 https://aws.amazon.com/ec2/ 5 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 04/10/2025 05/10/2025 https://aws.amazon.com/ec2/ Week 1 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/AWS-fcj/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nOn this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: AWS IAM, EC2 deployment, VPC networking, and Site-to-Site VPN fundamentals\nWeek 3: High Availability, Scalability, RDS, Aurora, ElastiCache, Route 53, and Solutions Architecture\nWeek 4: Amazon S3 storage, CloudFront, Global Accelerator, Storage Gateway, FSx, and integration services (SQS, SNS, Step Functions)\nWeek 5: Containers on AWS (ECS, EKS), Serverless Architecture (Lambda, API Gateway), and Database services (RDS, DynamoDB, Redshift, ElastiCache)\nWeek 6: Data \u0026amp; Analytics (Athena, Glue, Kinesis), Machine Learning (SageMaker, Rekognition), Monitoring (CloudWatch, CloudTrail), and Advanced IAM\nWeek 7: AWS Security \u0026amp; Encryption (KMS, CloudHSM, Shield, WAF), VPC Networking, and Disaster Recovery \u0026amp; Backup strategies\nWeek 8: Advanced Solutions Architecture, IoT Core, Ground Station, RoboMaker, and AWS Whitepapers\nWeek 9: Final AWS Cloud Journey summary report, full AWS system architecture design, and review of Week 1-8\nWeek 10: Project Proposal completion and AWS Architecture Diagram design\nWeek 11: Week 11 Worklog\nWeek 12: Week 12 Worklog\n"
},
{
	"uri": "http://localhost:1313/AWS-fcj/5-workshop/5.7-security/5.7.1-s3-cloudfront/",
	"title": "Configure S3 &amp; CloudFront",
	"tags": [],
	"description": "",
	"content": "1. Create S3 Bucket:\nCreate Bucket (Ex: minimarket-assets-prod) Block Public Access: On Manually upload images folder from code to this Bucket 2. Create CloudFront Distribution:\nOrigin type: Select Elastic Load Balancer Origin Domain: Select Load Balancer of Beanstalk Settings: Select Customize origin settings Protocol: HTTP Only Cache settings: Select Customize cache settings Viewer Protocol Policy: Redirect HTTP to HTTPS 3. Add S3 Origin (To retrieve images):\nGo to newly created Distribution Go to Origins tab \u0026gt; Create Origin Origin domain select the S3 created earlier (minimarket-assets-prod) Origin Access: Select Origin access control (OAC) \u0026gt; Create new OAC Bucket Policy: Copy policy provided by CloudFront and paste into S3 Bucket policy 4. Configure Behavior:\nReturn to CloudFront go to Behaviors tab Create Behavior with Path pattern: /images/ Point to Origin S3 Cache Policy: CachingOptimized "
},
{
	"uri": "http://localhost:1313/AWS-fcj/5-workshop/5.6-cicd/5.6.1-codebuild/",
	"title": "Create Build Project",
	"tags": [],
	"description": "",
	"content": " Access CodeBuild \u0026gt; Create project\nProject name: MiniMarket-Build\nSource: Select GitHub (Connect to Repo containing code)\nEnvironment:\nEnvironment image: Managed Image Operating system: Amazon Linux Runtime: Standard Image: 5.0 Service role: New service role Privileged: Enable (Required to run Docker build commands) Buildspec: Use a buildspec file\nClick Create build project\nAfter creation is complete, go to IAM Role of the newly created CodeBuild, grant additional permission AmazonEC2ContainerRegistryPowerUser so it can push images to ECR\n"
},
{
	"uri": "http://localhost:1313/AWS-fcj/5-workshop/5.3-network/5.3.1-create-vpc/",
	"title": "Create VPC &amp; Subnets",
	"tags": [],
	"description": "",
	"content": " Open Amazon VPC console (Note: Choose region suitable for needs, here the group uses Region ap-southeast-1) Select Create VPC Configuration: Name: MiniMarket-VPC IPv4 CIDR: 10.0.0.0/16 Create Subnets (Split 2 AZs to ensure High Availability): Public Subnets (2): 10.0.1.0/24 \u0026amp; 10.0.2.0/24 (Used for Load Balancer \u0026amp; NAT) Private Subnets (2): 10.0.3.0/24 \u0026amp; 10.0.4.0/24 (Used for App, DB, Redis) Click Create VPC and wait for state to change to Available is successful "
},
{
	"uri": "http://localhost:1313/AWS-fcj/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Introduction MiniMarket is an e-commerce application built on the .NET Core platform, applying modern 3-Tier Architecture. The goal of this Workshop is to re-platform the application from an On-premise environment to AWS cloud infrastructure (Cloud Native Migration) ensuring AWS Well-Architected Framework criteria: Security, Reliability, Performance Efficiency, and Cost Optimization Workshop Overview Solution Architecture:\nCompute: Use AWS Elastic Beanstalk (Docker platform) to simplify deployment, infrastructure management, and Auto Scaling Database: Amazon RDS for SQL Server deployed in Private Subnet to ensure data security Caching: Amazon ElastiCache (Redis) helps store User Sessions and offload Database queries, increasing response speed Network \u0026amp; Security: VPC: Designed with Public/Private Subnet model combined with NAT Gateway Application Layer Security: Use AWS WAF combined with Amazon CloudFront to protect against Web attacks and distribute content globally Storage: Amazon S3 used to store and serve static assets (product images) with high durability DevOps: Fully automated CI/CD process with AWS CodePipeline and CodeBuild Monitoring: Amazon CloudWatch to monitor system health (CPU, Network) and send alerts "
},
{
	"uri": "http://localhost:1313/AWS-fcj/5-workshop/5.8-monitoring/5.8.1-cloudwatch/",
	"title": "Monitoring with CloudWatch",
	"tags": [],
	"description": "",
	"content": " Create SNS Topic:\nGo to SNS \u0026gt; Topics \u0026gt; Create Topic Type: Standard Name: DevOps-Alerts Create Subscription\nCreate Subscription \u0026gt; Protocol: Email \u0026gt; Enter your email (Remember to Confirm mail) Create CPU Alarm:\nGo to CloudWatch \u0026gt; Alarms \u0026gt; Create alarm Select metric \u0026gt; EC2 \u0026gt; Per-Instance Metrics \u0026gt; Select InstanceID of Beanstalk \u0026gt; CPUUtilization Condition: CPUUtilization: Greater than 70% Notification: Select Topic DevOps-Alerts Create Alarm. "
},
{
	"uri": "http://localhost:1313/AWS-fcj/5-workshop/5.5-app/5.5.1-dockerize/",
	"title": "Package with Docker",
	"tags": [],
	"description": "",
	"content": "Before moving to Cloud, we need to package the .NET Core application into a Docker Image\nCreate Dockerfile: At the root directory of the Solution, create a file named Dockerfile (no extension) ```dockerfile # STAGE 1: BUILD FROM mcr.microsoft.com/dotnet/sdk:8.0 AS build WORKDIR /src COPY [\u0026quot;MiniMarket.sln\u0026quot;, \u0026quot;./\u0026quot;] COPY [\u0026quot;WebShop/WebShop.csproj\u0026quot;, \u0026quot;WebShop/\u0026quot;] # ... (Copy other projects if any) RUN dotnet restore \u0026quot;MiniMarket.sln\u0026quot; COPY . . WORKDIR \u0026quot;/src/WebShop\u0026quot; RUN dotnet build \u0026quot;WebShop.csproj\u0026quot; -c Release -o /app/build RUN dotnet publish \u0026quot;WebShop.csproj\u0026quot; -c Release -o /app/publish # STAGE 2: RUNTIME FROM mcr.microsoft.com/dotnet/aspnet:8.0 AS final COPY --from=build /app/publish . WORKDIR /app EXPOSE 8080 ENTRYPOINT [\u0026quot;dotnet\u0026quot;, \u0026quot;WebShop.dll\u0026quot;] ENV ASPNETCORE_URLS=http://+:8080 ENV ASPNETCORE_ENVIRONMENT=Development ``` Create buildspec.yml: Create file buildspec.yml to instruct AWS CodeBuild how to package and push to ECR ```yaml version: 0.2 phases: pre_build: commands: - echo Logging in to Amazon ECR... # --- INFORMATION CONFIGURATION --- - AWS_DEFAULT_REGION=ap-southeast-1 # Replace your Account ID in the line below: - AWS_ACCOUNT_ID= YOUR ACCOUNT ID - IMAGE_REPO_NAME=market-app - IMAGE_TAG=$(echo $CODEBUILD_RESOLVED_SOURCE_VERSION | cut -c 1-7) - REPOSITORY_URI=$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME # --------------------------- - aws ecr get-login-password --region $AWS_DEFAULT_REGION | docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com build: commands: - echo Build started on `date` - echo Building the Docker image... - docker build -t $REPOSITORY_URI:latest . - docker tag $REPOSITORY_URI:latest $REPOSITORY_URI:$IMAGE_TAG post_build: commands: - echo Build completed on `date` - echo Pushing the Docker image... - docker push $REPOSITORY_URI:latest - docker push $REPOSITORY_URI:$IMAGE_TAG - echo Writing image definitions file... # Automatically create Dockerrun.aws.json configuration file for Beanstalk # Map Port 80 (Host) to 8080 (Container .NET) - printf '{\u0026quot;AWSEBDockerrunVersion\u0026quot;:\u0026quot;1\u0026quot;,\u0026quot;Image\u0026quot;:{\u0026quot;Name\u0026quot;:\u0026quot;%s\u0026quot;,\u0026quot;Update\u0026quot;:\u0026quot;true\u0026quot;},\u0026quot;Ports\u0026quot;:[{\u0026quot;ContainerPort\u0026quot;:8080,\u0026quot;HostPort\u0026quot;:80}]}' \u0026quot;$REPOSITORY_URI:$IMAGE_TAG\u0026quot; \u0026gt; Dockerrun.aws.json - cat Dockerrun.aws.json artifacts: files: - Dockerrun.aws.json ``` "
},
{
	"uri": "http://localhost:1313/AWS-fcj/5-workshop/5.9-cleanup/5.9.1-cleanup/",
	"title": "Resource Cleanup",
	"tags": [],
	"description": "",
	"content": "To avoid unexpected costs after completing the Workshop, delete resources in the following correct order:\nNAT Gateway: Delete NAT Gateway \u0026gt; Wait for Deleted \u0026gt; Release Elastic IP (Most important as it costs the most) Elastic Beanstalk: Terminate Environment ElastiCache: Delete Redis Cluster (Uncheck Create Backup) RDS: Stop (or Delete if no longer in use - remember to uncheck Final Snapshot). WAF: Manage resources \u0026gt; Disassociate \u0026gt; Delete protection pack (web ACL) S3: Empty and Delete Bucket (Can skip deleting if still in use as cost is not too high)\nCloudFront Disable and Delete\n"
},
{
	"uri": "http://localhost:1313/AWS-fcj/5-workshop/5.4-data/5.4.1-security-groups/",
	"title": "Set up Security Groups",
	"tags": [],
	"description": "",
	"content": " Access EC2 \u0026gt; Security Groups \u0026gt; Create security group Group 1: Web Server (sg-web-app) Description: Allow HTTP from Internet Inbound Rules: Type: HTTP (80) | Source: 0.0.0.0/0 (Or only from Load Balancer if wanting stricter security) Group 2: Database (sg-db-sql) Description: Allow access only from Web Server Inbound Rules: Type: MSSQL (1433) | Source: Custom \u0026gt; Select ID of sg-web-app Group 3: Redis Cache (sg-redis-cache) Description: Allow access only from Web Server Inbound Rules: Type: Custom TCP (6379) | Source: Custom \u0026gt; Select ID of sg-web-app "
},
{
	"uri": "http://localhost:1313/AWS-fcj/5-workshop/5.3-network/5.3.2-gateways/",
	"title": "Configure Internet &amp; NAT Gateway",
	"tags": [],
	"description": "",
	"content": "Create Internet Gateway In the VPC dashboard click on Internet gateways Then click on Create internet gateway In the Internet gateway creation section, name it as desired then click on Create internet gateway and wait for it to be created After the Internet gateway is finished creating go to Actions and click on Attach to VPC to attach it to the VPC created in the previous section Create NAT Gateway Create NAT Gateway placed in Public Subnet 1 Assign Elastic IP to have a static address to the Internet "
},
{
	"uri": "http://localhost:1313/AWS-fcj/5-workshop/5.3-network/5.3.3-routing/",
	"title": "Configure Route Table",
	"tags": [],
	"description": "",
	"content": "Create Route Table Click on Route tables section in VPC dashboard\nCreate 2 Route Tables, Public with Private\nPublic Route Table: For Public Route Table, in Routes section click Edit routes Point 0.0.0.0/0 to Internet Gateway And in Subnet associations section, assign to both Public Subnets Private Route Table: For Private Route Table, we will point 0.0.0.0/0 to NAT Gateway And in Subnet associations section, assign to both Private Subnets Separating Route Tables ensures that Databases in Private Subnet are never exposed directly to the Internet.\n"
},
{
	"uri": "http://localhost:1313/AWS-fcj/5-workshop/5.4-data/5.4.2-rds/",
	"title": "Initialize Amazon RDS",
	"tags": [],
	"description": "",
	"content": " Access RDS Console \u0026gt; Subnet groups \u0026gt; Create DB subnet group Name: db-private-group Subnets: Select 2 AZs and select exactly 2 Private Subnets Go to Databases \u0026gt; Create database Engine options: Microsoft SQL Server (Express Edition) Templates: Free tier Settings: Set Master Password (remember for later use) Connectivity: VPC: VPC you created for Web Subnet group: db-private-group Public access: No VPC security group: Select Security group you created for database Click Create database "
},
{
	"uri": "http://localhost:1313/AWS-fcj/5-workshop/5.5-app/5.5.2-beanstalk-setup/",
	"title": "Initialize Elastic Beanstalk",
	"tags": [],
	"description": "",
	"content": "We will create an environment to run the application\nAccess Elastic Beanstalk \u0026gt; Create application App Name: MiniMarket-App Platform: Docker (Amazon Linux 2023) Application code: Select Sample application (To test infrastructure first) Network Configuration (Networking) - Extremely Important: VPC: Select the VPC you created for MiniMarket Instance settings: Public IP address: Uncheck Subnets: Select 2 Private Subnets EC2 security groups: Select sg-web-app Capacity: Environment type: Select Load balanced Load balancer network settings: Visibility: Public Subnets: Select 2 Public Subnets Click Create. The system will take about 5-7 minutes to initialize "
},
{
	"uri": "http://localhost:1313/AWS-fcj/5-workshop/5.2-prerequiste/",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": "IAM permissions Attach AdministratorAccess in IAM permission policy to the AWS account for easier workflow Note: Using Administrator privileges is recommended only for the Workshop environment to ensure the deployment process is uninterrupted. In a real Production environment, adhere to the Least Privilege principle for each service { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Source Code GitHub repository containing .NET Core code and a valid Dockerfile "
},
{
	"uri": "http://localhost:1313/AWS-fcj/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "Digital Transformation for Mini-market on AWS Cloud .NET E-commerce 3-tier Solution Applying Repository and Unit of Work Pattern 1. Executive Summary This proposal presents an end-to-end solution for \u0026ldquo;Digital Transformation for Mini-market on the AWS Cloud.\u0026rdquo; Traditional mini-markets are currently facing three major challenges: (1) Manual inventory management (using Excel/notebooks) leading to revenue loss and resource waste; (2) 100% reliance on offline sales, missing the fast-growing e-commerce market and losing competitiveness; and (3) Slow operational processes (such as manual price lookup), resulting in poor customer experience.\nThe proposed solution is to build a comprehensive e-commerce and operational management platform. For software architecture, the team will use a .NET 3-layer architecture (ASP.NET Core MVC, EF Core) combined with the Repository Pattern and Unit of Work Pattern. For infrastructure architecture, the system is designed following the AWS Well-Architected Framework, running on AWS Elastic Beanstalk (for the .NET backend), Amazon RDS for SQL Server (for the database), and Amazon S3 (for static assets). The system is performance-optimized using CloudFront and ElastiCache, and secured using WAF, VPC, and NAT Gateway. The deployment process is fully automated using a CI/CD pipeline integrated with GitHub.\nThe business benefits are immediate, including automated inventory management (reducing losses) and opening a new online revenue channel. In terms of investment, infrastructure cost in the first 12 months is nearly zero thanks to AWS Free Tier (e.g., RDS Express Edition, EC2 t3.micro). The long-term operational cost (after Free Tier) is also very reasonable, estimated at only 138.06 USD/month for the entire system. With minimal initial investment and the ability to directly address revenue leakage issues, the ROI is very high and nearly instantaneous.\nThe project is proposed to be implemented within 11 weeks, divided into four main phases: (1) Foundation \u0026amp; Architecture, (2) Core Feature Development, (3) AWS Integration \u0026amp; CI/CD, and (4) Finalization \u0026amp; Deployment. Expected outcomes are measured using specific success metrics: reducing inventory errors by 90%, reducing checkout time by 50%, and achieving 20% online revenue within the first 6 months. This solution not only addresses immediate problems but also provides a scalable platform for future data-driven decision-making.\n2. Problem Statement Current Situation\nSmall and medium retail businesses, especially traditional mini-market models in Vietnam, are operating based on outdated manual processes. As the market becomes increasingly digital, the lack of technology adoption has created multiple issues that directly impact their ability to survive and grow.\nKey Problems\nManual inventory management leads to inaccurate data and resource waste:\nMost mini-markets currently manage thousands of SKUs using notebooks or Excel files. Stock-in/out and end-of-day inventory checks rely entirely on manual counting and data entry. This easily leads to data errors due to frequent mistakes in product codes or quantities, causing large discrepancies between “recorded” and “actual” stock. Manual checking also requires significant labor, as employees spend hours daily counting, reconciling, and correcting reports instead of focusing on sales or customer service. Ultimately, this results in financial losses, as inaccurate data prevents owners from monitoring expired goods, damaged items, or theft, typically causing 5–10% inventory loss monthly.\nDependence on offline sales, missing the E-commerce market:\nMost mini-markets in Vietnam depend heavily on walk-in customers. They are limited by geography (serving only nearby areas) and a small group of familiar customers. Operating offline only means missing out on the growing e-commerce customer base, especially younger generations used to online shopping. They cannot compete with the convenience of 24/7 ordering or home delivery offered by major convenience store chains like Circle K or 7-Eleven and delivery apps like Grab or Shopee, resulting in customer loss over time.\nOperational inefficiency and poor customer experience:\nCheckout and information lookup processes in traditional mini-markets are usually very slow. When customers ask for product prices, details, or promotions, employees (especially new ones) must search manually, wasting time. Making customers wait long causes frustration and appears unprofessional. Employees spend too much time on simple, error-prone tasks (e.g., misreading handwritten prices), reducing the number of customers served during peak hours.\n3. Solution Architecture The architecture is designed to address the problems above by combining a .NET 3-tier software architecture with AWS managed cloud services. This architecture follows the principles of the AWS Well-Architected Framework, ensuring security, high performance, fault tolerance, and cost optimization.\nAWS Services Used\nAWS Elastic Beanstalk: A Platform as a Service (PaaS) chosen to deploy the .NET 3-tier application (including the WebShop Presentation Layer and Application Services Layer). Beanstalk automates 100% of infrastructure management, including automatically creating an Auto Scaling Group (ASG) to ensure scalability and cost efficiency.\nAmazon RDS (SQL Server): A Managed Relational Database Service that hosts the Persistence Layer. SQL Server is chosen because the .NET application has been developed and optimized for SQL Server. Using RDS for SQL Server allows migrating the application to AWS without modifying the Data Layer code. RDS automates daily backups, patching, and failover. For security, RDS is placed in a Private Subnet, preventing direct Internet access and allowing only the Beanstalk application to connect. For cost optimization, SQL Server Express Edition on RDS (Free Tier eligible) is used during the early phase.\nAmazon S3: Object storage used for static assets such as product images, CSS files, and JavaScript files. S3 provides extremely low cost and unlimited scalability.\nAmazon CloudFront: A Content Delivery Network (CDN). CloudFront caches static files from S3 at global edge locations, significantly improving page load speed and reducing load on Beanstalk, allowing the .NET application to focus on business logic.\nAmazon WAF \u0026amp; Route 53: WAF (Web Application Firewall) is integrated with CloudFront to block common attacks (SQL injection, XSS). Route 53 provides domain and DNS routing.\nAmazon ElastiCache (Redis): An in-memory data store. It reduces load on RDS for repeated queries (e.g., homepage product list). The .NET application will cache hot data in Redis to improve response times. Like RDS, ElastiCache is placed in a Private Subnet for security.\nNAT Gateway: Provides secure outgoing Internet access for private resources (such as Elastic Beanstalk), allowing servers to download security patches without being exposed to the Internet.\nAWS CodePipeline/CodeBuild: CI/CD services that integrate with GitHub to automate: (1) CodeBuild compiles the .NET code, (2) CodePipeline deploys new versions to Elastic Beanstalk.\nData Flow\n[1]-[2] Users access the domain (via Route 53) and are routed to CloudFront. WAF filters the request. [3] (Static Flow) If static files (images, CSS), CloudFront serves directly from Amazon S3. [4]-[6] (Dynamic Flow) If dynamic requests, CloudFront forwards traffic through the Internet Gateway to the Application Load Balancer, then to Elastic Beanstalk. [7]-[8] The .NET application checks ElastiCache first; if not available, it queries Amazon RDS. [9]-[10] When Elastic Beanstalk requires Internet access (e.g., downloading patches), it communicates through NAT Gateway to the Internet Gateway. [11]-[14] (CI/CD Flow) When developers push code to GitHub, CodePipeline and CodeBuild automatically build and deploy the new version to Elastic Beanstalk. 4. Technical Implementation Phases of Implementation\nThe project is divided into 4 main phases over 11 weeks to ensure progress and quality:\nBuilding technical foundation: Focus on setting up the technical foundation, finalizing data models for core entities, establishing the .NET 3-tier solution structure (Domain, Application, Persistence, WebShop), initializing the GitHub repository, and learning AWS services. (Week 1–4)\nDeveloping core features: Complete the Persistence Layer (Repositories, Unit of Work) and Application Layer (Services) for core tasks such as product, user, and order management. Meanwhile, the WebShop Layer (Controllers, Views) will be built for login, cart, checkout flows, and Unit Tests for Services will begin. (Week 5–7)\nIntegrating AWS services: Integrate Amazon S3 for product images and ElastiCache (Redis) for caching. The team will complete the CI/CD pipeline for automatic deployment to the Staging environment on Elastic Beanstalk and conduct Integration Testing. (Week 8–10)\nFinalization and deployment: Configure security services such as CloudFront, WAF, and Route 53. Deploy version 1.0 to Elastic Beanstalk, conduct final UAT, and set up monitoring using CloudWatch. (Week 11)\nTechnical Requirements\nBackend: ASP.NET Core MVC 9.0 ORM: Entity Framework Core Database: MS SQL Server 2022 (Local) and Amazon RDS for SQL Server (Cloud) Frontend: Bootstrap 5, jQuery, Bootstrap Icons Cloud Platform (AWS): Elastic Beanstalk, RDS, S3, CloudFront, WAF, Route 53, ElastiCache, VPC, NAT Gateway, CodePipeline, CodeBuild Source Control: Git Tools: Visual Studio 2022, Docker Desktop Development Methodology\nAgile (Scrum-like) is applied for flexibility and progress assurance, aligned with the four planned phases. All tasks (features, bugs) are tracked via a Kanban board. All new code must be reviewed via GitHub merge requests before merging into the main branch to maintain code quality.\nTesting Strategy\nThree levels of testing will be executed:\nUnit Testing (100% focused on Application Layer, using mocked repositories) Integration Testing (on Staging with real RDS and EF Core) User Acceptance Testing (on Production UI for flows like Register, Login, Checkout) Deployment Plan\nA fully automated CI/CD pipeline is used. When code is pushed to GitHub, AWS CodePipeline triggers CodeBuild to compile the project, run Unit Tests, and package a .zip file. If successful, CodePipeline deploys the new version to Staging on Elastic Beanstalk.\n5. Roadmap \u0026amp; Milestones The project is planned over 11 weeks, divided into 4 phases:\nPhase 1 (Week 1–4): Technical foundation (data models, .NET architecture, GitHub, AWS VPC/Subnets).\nMilestone: Architecture \u0026amp; repositories completed.\nPhase 2 (Week 5–7): Core features (Products, Orders, Auth, Cart, Checkout, Unit Tests).\nMilestone: All core flows working locally.\nPhase 3 (Week 8–10): AWS integration (S3, Redis, CI/CD, Staging deployment).\nMilestone: Pipeline working, Staging deployment stable.\nPhase 4 (Week 11): Security configuration, Production deployment, UAT, CloudWatch monitoring.\nMilestone: Version 1.0 live on Production.\n6. Budget Estimate View full estimate:\nAWS Pricing Calculator\nOr download the budget file.\nInfrastructure Cost\nThe AWS Pricing Calculator estimates monthly operational cost at 138.06 USD with 0.00 USD upfront. The cost optimization strategy focuses on maximizing AWS Free Tier and managed services.\nBreakdown:\nEC2 t3a.small (via Beanstalk): $19.15/month ALB: $18.69/month VPC (NAT Gateway): $46.02/month RDS SQL Server Express (db.t3.micro): $25.86/month ElastiCache (t4g.micro): $17.52/month WAF: $7.20/month CloudFront, S3, Route 53, CodeBuild: small remaining cost Most core services fall under Free Tier during the first 12 months, reducing real costs significantly (mainly NAT Gateway + WAF remain).\nROI is extremely high because the solution directly reduces inventory loss and opens new revenue channels.\n7. Risk Assessment Three types of risks were identified: Technical, Business, and Operational.\nTechnical: System Overload (Performance Bottleneck)\nImpact: High | Probability: Medium Mitigation: Redis caching, Auto Scaling, CloudFront caching Contingency: CloudWatch alarms, immediate RDS vertical scaling Business: Low User Adoption\nImpact: High | Probability: Medium Mitigation: Simple UI, early feedback, training materials Contingency: Extra Sprint to adjust features based on feedback Operational: Data Loss / Breach\nImpact: Critical | Probability: Low Mitigation: Automated RDS backups, Private Subnet isolation, WAF protection, secured environment variables Contingency: Point-in-Time Recovery (PITR) 8. Expected Outcomes Business metrics:\n90% reduction in inventory errors 50% faster checkout time 20% online revenue within 6 months Technical metrics:\n99.9% uptime \u0026lt;2s page load time (CloudFront + Redis) Stable CI/CD deployment frequency Short-term (0–6 months):\nImmediate UX improvements Inventory automation Reduced operational errors Medium-term (6–18 months):\nMarket expansion Collection of valuable business data Long-term:\nFull data-driven decision-making Easy multi-store scaling via Elastic Beanstalk + RDS "
},
{
	"uri": "http://localhost:1313/AWS-fcj/5-workshop/5.6-cicd/5.6.2-codepipeline/",
	"title": "Set up CodePipeline",
	"tags": [],
	"description": "",
	"content": " Access CodePipeline \u0026gt; Create pipeline\nCategory: Select Build custom pipeline\nSettings: Select New service role\nSource Stage: Select GitHub (via GitHub App) \u0026gt; Connect to GitHub \u0026gt; Select Repo and branch that you deploy to cloud\nBuild Stage: Select AWS CodeBuild \u0026gt; Select project MiniMarket-Build just created\nTest Stage: Click Skip test stage\nDeploy Stage:\nProvider: AWS Elastic Beanstalk Application name: MiniMarket-App Environment name: Select running environment Click Create pipeline\nIf Deploy step has Permission error, go to IAM Role of CodePipeline and grant permission AdministratorAccess-AWSElasticBeanstalk\n"
},
{
	"uri": "http://localhost:1313/AWS-fcj/5-workshop/5.7-security/5.7.2-waf/",
	"title": "Set up Firewall (WAF)",
	"tags": [],
	"description": "",
	"content": " Access WAF \u0026amp; Shield \u0026gt; Protection packs (webs ACLs) \u0026gt; Create protection pack (web ACL) App category: E-commerce \u0026amp; transaction platforms App focus: Both API and web Add resources \u0026gt; Add CloudFront or Amplify resources \u0026gt; Select CloudFront distribution created in previous section Choose initial protections \u0026gt; Build your own pack from all of the protections AWS WAF offers \u0026gt; AWS-managed rule group:\nAdd Core rule set (Block bot, bad IP) Add SQL database (Block SQL Injection) Testing: Access URL: https://[domain]/?id=1 OR 1=1. If receiving error 403 Forbidden, WAF is active.\n"
},
{
	"uri": "http://localhost:1313/AWS-fcj/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Goals Learn and apply AWS IAM for identity and access control. Deploy and manage EC2 instances. Explore Amazon VPC and AWS Site-to-Site VPN networking fundamentals. Reinforce C# programming basics. Time Period: 06/10/2025 - 12/10/2025\nWeekly Tasks Overview Day Activity Start Date End Date Reference 1 - Studied IAM fundamentals + Created and assigned users, groups, roles + Attached permission policies for least-privilege access + Verified access through IAM Console 06/10/2025 06/10/2025 https://000002.awsstudygroup.com/ 2 - Deployed and configured an EC2 instance + Selected Amazon Linux 2023 (Free Tier) + Configured key pair and security group (SSH) + Connected via SSH and validated status + Practiced VPC networking basics (subnets, route tables, Internet Gateway) 07/10/2025 07/10/2025 https://000003.awsstudygroup.com/ 3 - Explored Amazon Bedrock Playground + Tested models like Claude 3 Haiku and Titan Text + Wrote sample prompts and evaluated responses + Tried varied scenarios to observe model behavior 08/10/2025 08/10/2025 https://cloudjourney.awsstudygroup.com/ 4 - Reviewed C# fundamentals + Revisited control flow, loops, classes, and objects + Wrote small sample programs to validate logic + Summarized key AWS services with ChatGPT for quick recall 09/10/2025 10/10/2025 - Week 2 Achievements Applied IAM principles in practice (users, roles, and access policies).\nImplemented least-privilege permissions and appropriate group policies. Understood relationships between users, groups, roles, and policies. Successfully launched, connected to, and terminated EC2 instances within the Free Tier.\nManaged instance lifecycle from the EC2 Console. Verified SSH connectivity and basic instance health. Built foundational knowledge of Amazon VPC:\nSubnets, Route Tables, Internet Gateway, NAT Gateway, and logical network design. Understood routing mechanics and secure network isolation. Learned AWS Site-to-Site VPN concepts for secure connectivity between on-premises networks and VPCs.\nGrasped VPN tunnel behavior and high-level setup steps. Gained hands-on exposure to Amazon Bedrock Playground (Claude/Titan).\nPracticed prompt writing to obtain accurate, useful responses. Explored generative AI use cases and response patterns. Reinforced C# programming alongside AWS exercises.\nAuthored small snippets to validate syntax and core logic. Formed a broader view of primary AWS domains:\nCompute (EC2, Lambda), Networking (VPC, VPN), AI/ML (Bedrock). Understood how these services combine to build end-to-end cloud solutions. "
},
{
	"uri": "http://localhost:1313/AWS-fcj/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nThis section will list and introduce the blogs you have translated. For example:\nBlog 1 - Getting started with healthcare data lakes: Using microservices This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 2 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 3 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 4 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 5 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 6 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"
},
{
	"uri": "http://localhost:1313/AWS-fcj/5-workshop/5.5-app/5.5.3-app-config/",
	"title": "Configure Environment Variables",
	"tags": [],
	"description": "",
	"content": "To enable the application to connect to Database and Redis, we do not hardcode in the code but use Environment Variables\nGo to Beanstalk Environment \u0026gt; Configuration \u0026gt; Updates, monitoring, and logging \u0026gt; Edit\nScroll down to Environment properties section\nAdd the following variables:\nName: ConnectionStrings__DefaultConnection\nValue: Server=sql-shop-db\u0026hellip;.rds.amazonaws.com;Database=MiniMarketDB;User Id=admin;Password=PASSWORD YOU SET;TrustServerCertificate=True; Name: ConnectionStrings__RedisConnection\nValue: webapp.redis.cache\u0026hellip;:6379 Name: VnPay__IPNUrl\nValue: https://[cloudfrontdomain].cloudfront.net/Payment/VnPayIPN Name: VnPay__ReturnUrl\nValue: https://[cloudfrontdomain].cloudfront.net/Payment/VnPayReturn Click Apply. Server will restart to apply new configuration "
},
{
	"uri": "http://localhost:1313/AWS-fcj/5-workshop/5.4-data/5.4.3-elasticache/",
	"title": "Initialize ElastiCache Redis",
	"tags": [],
	"description": "",
	"content": " Access ElastiCache \u0026gt; Subnet groups \u0026gt; Create subnet group Name: redis-private-group Subnets: Select 2 Private Subnets Go to Redis OSS caches \u0026gt; Create cache At Cluster settings screen: Engine: Select Redis OSS Deployment option: Select Node-based cluster Creation method: Select Cluster cache (Configure and create a new cluster) Cluster mode: Select Disabled (Simple mode, 1 Shard) At Location screen:\nLocation: AWS Cloud Multi-AZ: Uncheck (Enable) Note: Disable this feature to save costs for Lab environment Auto-failover: Uncheck (Enable) At Cache settings screen:\nEngine version: Leave default (Ex: 7.1) Port: 6379 Node type: Select t3 family \u0026gt; Select cache.t3.micro Number of replicas: Enter 0 (We only need 1 primary node, no replica node needed) At Connectivity screen: Network type: IPv4 Subnet groups: Select Choose existing subnet group \u0026gt; Select redis-private-group just created At Advanced settings screen (Important): Encryption at rest: Enable (Default) Encryption in transit: Uncheck (Disable) Reason: Disabling encryption in transit simplifies connection from .NET code in internal VPC environment without configuring complex SSL certificates Selected security groups: Select Manage \u0026gt; select sg-redis-cache (Uncheck default) Scroll to the bottom and click Create 3. Get connection information Initialization process will take about 5-10 minutes\nWhen status changes to Available (Green) Click on Cluster name (webapp or name you set) At Overview tab, find Primary endpoint Copy this connection string (Ex: webapp.xxxx.cache.amazonaws.com) This Endpoint will be used to configure ConnectionStrings__RedisConnection environment variable for Elastic Beanstalk in the following steps.\n"
},
{
	"uri": "http://localhost:1313/AWS-fcj/5-workshop/5.3-network/",
	"title": "Network Infrastructure Setup",
	"tags": [],
	"description": "",
	"content": "Overview In this section, we will build the network foundation for the MiniMarket application. A secure network architecture is a prerequisite to protect the application and data\nWe will design a VPC consisting of:\nPublic Subnet: Dedicated to components communicating directly with the Internet (Load Balancer, NAT Gateway). Private Subnet Dedicated to components requiring security (App Server, Database, Redis) Additionally, we will configure NAT Gateway to allow servers inside the Private Subnet to download updates and Docker Images from the Internet without exposing IP addresses externally\nContent Create VPC \u0026amp; Subnet Configure Internet \u0026amp; NAT Gateway Configure Route Table "
},
{
	"uri": "http://localhost:1313/AWS-fcj/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Goals Understand the fundamentals of High Availability (HA) and Scalability in cloud environments. Explore AWS managed database services including RDS, Aurora, and ElastiCache. Learn how to configure and manage Amazon Route 53 for DNS and routing control. Study common AWS Solution Architectures and their best-practice design patterns. Time Period: 13/10/2025 - 19/10/2025\nWeekly Tasks Overview Day Activity Start Date End Date Reference 1 - Studied High Availability \u0026amp; Scalability concepts + Reviewed Multi-AZ deployment models + Configured sample Auto Scaling Group + Understood the role of Elastic Load Balancing (ELB) in distributing traffic 09/22/2025 09/22/2025 https://aws.amazon.com/autoscaling/ 2 - Explored Amazon RDS and Aurora + Created RDS instance in Multi-AZ mode + Tested read replicas and failover + Learned Aurora clustering and automatic recovery 09/23/2025 09/23/2025 https://aws.amazon.com/rds/ 3 - Worked with Amazon ElastiCache + Compared Redis vs Memcached engines + Practiced creating cache clusters + Monitored performance metrics through CloudWatch 09/24/2025 09/24/2025 https://aws.amazon.com/elasticache/ 4 - Practiced Amazon Route 53 configuration + Created hosted zones and DNS records + Set up failover routing + Tested latency-based routing between regions 09/25/2025 09/25/2025 https://aws.amazon.com/route53/ 5–6 - Learned about Classic Solutions Architecture + Analyzed multi-tier and serverless designs + Focused on HA \u0026amp; scalable patterns used in production + Designed a small-scale architecture combining EC2, RDS, and ELB 09/26/2025 09/27/2025 https://aws.amazon.com/architecture/ Week 3 Achievements Gained hands-on understanding of High Availability strategies:\nImplemented Multi-AZ replication and automatic failover. Configured Auto Scaling Groups for dynamic resource adjustment. Used Elastic Load Balancing to maintain traffic distribution and uptime. Developed strong knowledge of AWS Database Services:\nManaged RDS instances and explored Aurora’s performance benefits. Understood how ElastiCache improves application responsiveness through in-memory caching. Mastered essential DNS and routing concepts with Amazon Route 53:\nBuilt hosted zones, routing policies, and health checks. Tested failover and latency-based routing scenarios. Strengthened architectural design thinking by studying Classic AWS Solutions:\nLearned how to combine compute, database, and networking services for resilient systems. Designed a small-scale architecture focusing on fault tolerance and scalability. Overall, consolidated skills across Compute, Database, Networking, and Architecture Design, preparing for advanced AWS infrastructure modules.\n"
},
{
	"uri": "http://localhost:1313/AWS-fcj/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim for your report, including this warning.\nIn this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3…, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event’s content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"
},
{
	"uri": "http://localhost:1313/AWS-fcj/5-workshop/5.4-data/",
	"title": "Data Layer Deployment",
	"tags": [],
	"description": "",
	"content": "Overview Data is the most important asset of every system. Therefore we will set up the data layer (Data Layer) for MiniMarket with criteria: Maximum Security and High Performance\nWe will deploy two core services:\nAmazon RDS (Relational Database Service): Use SQL Server to store business data (Products, Orders, Users). Database will be placed in Private Subnet to prevent direct access from the Internet Amazon ElastiCache (Redis): Use Redis as cache memory (In-memory Cache) to store Login Sessions and offload queries for the main Database Content Set up Security Groups for DB \u0026amp; Cache Initialize Amazon RDS (SQL Server) Initialize Amazon ElastiCache (Redis) "
},
{
	"uri": "http://localhost:1313/AWS-fcj/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Goals Understand and optimize data storage using Amazon S3. Advance skills in S3 management, security, and performance optimization. Learn to use CloudFront and Global Accelerator for global content delivery and latency reduction. Explore additional AWS storage solutions such as FSx, Storage Gateway, and service integration using SQS, SNS, and Step Functions. Time Period: 20/10/2025 - 26/10/2025\nWeekly Tasks Overview Day Activity Start Date End Date Reference 1 - Studied Amazon S3 Basics + Created buckets and uploaded sample files + Compared storage tiers (Standard, Infrequent Access, Glacier) + Analyzed cost optimization for each tier 20/10/2025 20/10/2025 https://aws.amazon.com/s3/ 2 - Practiced Advanced S3 Management + Configured Cross-Region Replication (CRR) between two regions + Set up Lifecycle Policies for automatic data transition to Glacier + Enabled Versioning and tested object recovery 21/10/2025 21/10/2025 https://docs.aws.amazon.com/AmazonS3/ 3 - Implemented S3 Security Best Practices + Applied IAM policies and Bucket Policies for restricted access + Configured Server-Side Encryption (SSE) and AWS KMS + Verified Block Public Access settings to ensure privacy 22/10/2025 22/10/2025 https://docs.aws.amazon.com/s3/security/ 4 - Explored CloudFront \u0026amp; Global Accelerator + Configured a CloudFront Distribution to serve content from S3 + Integrated Global Accelerator for global low-latency access + Evaluated performance improvements through testing tools 23/10/2025 23/10/2025 https://aws.amazon.com/cloudfront/ 5 - Studied AWS Storage Extras + Learned about AWS Storage Gateway for hybrid storage + Configured FSx for Windows and FSx for Lustre + Compared performance between storage solutions 24/10/2025 24/10/2025 https://aws.amazon.com/storage/ 6 - Explored AWS Integration \u0026amp; Messaging + Practiced using SQS for message queuing + Implemented SNS for notifications + Set up Step Functions for workflow orchestration 25/10/2025 25/10/2025 https://aws.amazon.com/messaging/ Week 4 Achievements Built a strong foundation in Amazon S3 storage architecture:\nUnderstood key storage classes and their use cases. Analyzed cost-efficiency, durability, and retrieval time for each tier. Enhanced skills in S3 advanced management:\nSuccessfully configured Cross-Region Replication. Applied Lifecycle Policies for automated cost optimization. Enabled Versioning for data recovery and version control. Strengthened S3 Security knowledge:\nImplemented IAM and Bucket Policies for controlled access. Enabled encryption with SSE and KMS. Verified bucket security using Block Public Access settings. Practiced global distribution with CloudFront and Global Accelerator:\nDeployed a global CDN to deliver content efficiently. Understood how latency-based routing enhances user experience. Explored additional AWS Storage Solutions:\nLearned about Storage Gateway and hybrid storage concepts. Worked with FSx for Windows File Server and FSx for Lustre for high-performance workloads. Studied AWS Integration and Messaging services:\nUtilized SQS for asynchronous message handling. Sent alerts using SNS and automated processes via Step Functions. Gained comprehensive understanding across Storage, Security, Networking, and Integration, preparing for the upcoming Serverless \u0026amp; Application Deployment modules.\n"
},
{
	"uri": "http://localhost:1313/AWS-fcj/5-workshop/5.5-app/",
	"title": "Application Deployment",
	"tags": [],
	"description": "",
	"content": "Overview After having network and data infrastructure, the next step is to bring .NET Core application source code to Cloud. Instead of managing each EC2 virtual server manually, we will use the Platform-as-a-Service (PaaS) platform which is AWS Elastic Beanstalk\nGoals of this module:\nContainerization: Package MiniMarket application into Docker Container to ensure uniform running environment (Dev = Prod) Deployment: Deploy Container to Elastic Beanstalk. The system will automatically provision EC2, configure Load Balancer and Auto Scaling Group Connectivity: Configure so application connects securely to RDS and Redis via Environment Variables Content Package application with Docker Initialize Elastic Beanstalk Environment Configure Database \u0026amp; Redis connection "
},
{
	"uri": "http://localhost:1313/AWS-fcj/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Deploying Cloud-Native MiniMarket Application on AWS Overview This workshop provides a comprehensive guide on re-platforming the MiniMarket e-commerce application (developed on the .NET Core platform) from a local environment to the AWS cloud infrastructure using a Cloud Native architecture.\nWe will not merely rent a virtual server (EC2) to run code. Instead, we will build a distributed system that is highly scalable, secure, and automated based on managed services.\nWe will establish a Multi-tier architecture consisting of core components:\nCompute: Use AWS Elastic Beanstalk combined with Docker to simplify application deployment and management, supporting automatic Auto Scaling based on traffic. Data \u0026amp; Caching: Migrate from local SQL Server to Amazon RDS (placed in Private Subnet) to ensure data security. Simultaneously, deploy Amazon ElastiCache (Redis) to manage User Sessions, ensuring high performance for the application. Networking \u0026amp; Security: Use VPC along with Public/Private Subnet and NAT Gateway for secure outbound connection, and protect the application against attacks using AWS WAF combined with CloudFront. DevOps: Build a CI/CD process using AWS CodePipeline and CodeBuild, allowing automation of the process from committing code to GitHub until the application runs in the Production environment Content Workshop Overview Prerequisites Network Infrastructure Setup (VPC, NAT, Security Groups) Data Layer Deployment (RDS \u0026amp; Redis) Application Deployment with Elastic Beanstalk \u0026amp; Docker Automation with CI/CD Pipeline Optimization and Security(S3, CloudFront, WAF) Monitoring (CloudWatch) Resource Cleanup "
},
{
	"uri": "http://localhost:1313/AWS-fcj/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Goals Learn how to deploy and manage containers on AWS using ECS, EKS, and Docker. Understand the fundamentals and advantages of Serverless Architecture. Build and design Serverless applications integrating Lambda, API Gateway, S3, and DynamoDB. Explore AWS database services including RDS, Aurora, DynamoDB, Redshift, and ElastiCache. Time Period: 27/10/2025 - 02/11/2025\nWeekly Tasks Overview Day Activity Start Date End Date Reference 1 - Studied Containers on AWS + Learned differences between ECS and EKS + Deployed a sample Docker container on ECS (Fargate) + Explored container orchestration concepts 27/10/2025 27/10/2025 https://aws.amazon.com/containers/ 2 - Practiced EKS (Elastic Kubernetes Service) basics + Created an EKS cluster and deployed a simple app + Connected local Docker images to ECR (Elastic Container Registry) + Monitored cluster via AWS Console 28/10/2025 28/10/2025 https://aws.amazon.com/eks/ 3 - Explored Serverless Overview + Learned the concepts of event-driven computing + Created and tested basic AWS Lambda functions + Integrated Lambda with API Gateway to expose endpoints 29/10/2025 29/10/2025 https://aws.amazon.com/lambda/ 4 - Designed Serverless Architectures + Connected Lambda with DynamoDB and S3 + Created a simple data pipeline using Step Functions + Tested an end-to-end serverless workflow 30/10/2025 30/10/2025 https://aws.amazon.com/architecture/serverless/ 5 - Studied Databases in AWS + Compared relational vs. NoSQL solutions + Created test databases in RDS and DynamoDB + Explored analytics with Redshift and caching using ElastiCache 31/10/2025 31/10/2025 https://aws.amazon.com/databases/ 6 - Reviewed all concepts learned during the week + Deployed a small demo project combining Lambda, API Gateway, DynamoDB, and S3 + Documented lessons learned and best practices 1/11/2025 1/11/2025 - Week 5 Achievements Gained hands-on experience with Containers on AWS:\nDeployed and managed Docker containers using ECS (Fargate) and EKS. Understood container orchestration, scaling, and image management via ECR. Developed foundational understanding of Serverless Architecture:\nBuilt and executed Lambda functions for event-driven workloads. Integrated Lambda with API Gateway to create RESTful endpoints. Designed and implemented a complete Serverless Application:\nConnected Lambda with DynamoDB and S3 to automate data processing. Used Step Functions for workflow orchestration and automation. Strengthened knowledge of AWS Database Services:\nWorked with RDS for relational databases and DynamoDB for NoSQL workloads. Understood Aurora’s scalability and Redshift’s analytics capabilities. Tested caching with ElastiCache to improve performance. Applied DevOps mindset by combining containers, serverless computing, and databases into a unified deployment workflow.\nPrepared for next week’s focus on Monitoring, CI/CD, and Infrastructure as Code (IaC) using AWS Developer Tools.\n"
},
{
	"uri": "http://localhost:1313/AWS-fcj/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nDuring my internship at AMAZON WEB SERVICES VIETNAM COMPANY LIMITED from 29/9/2025 to 22/11/2025, I had the opportunity to work directly on a real-world cloud transformation project titled “Digital Transformation for Mini-Market on AWS Cloud”. Throughout this internship, I applied the knowledge learned in school—software engineering, cloud computing, and system design—to build a complete .NET 3-tier e-commerce platform deployed on the AWS Cloud.\nI contributed to the development of key modules in the system architecture, including:\nDesigning and implementing Domain, Application, Persistence, and WebShop layers following the Repository \u0026amp; Unit of Work patterns\nBuilding core features such as product management, authentication, shopping cart, checkout flow\nIntegrating AWS services including S3, RDS, CloudFront, Elastic Beanstalk, ElastiCache, WAF, VPC, and CI/CD with CodePipeline\nSupporting the preparation of architecture diagrams, system documentation, and cost estimation\nParticipating in testing, performance analysis, and deployment to the staging/production environment\nBeyond technical skills, I developed communication, teamwork, documentation, and problem-solving abilities by working closely with supervisors and adapting to professional workflows.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Applied .NET, SQL Server, AWS services, Git, CI/CD, and system design into a real cloud architecture ✅ ☐ ☐ 2 Ability to learn Quickly understood new technologies such as Elastic Beanstalk, VPC, WAF, and CloudFront ☐ ✅ ☐ 3 Proactiveness Independently researched AWS solutions and solved technical issues during development ✅ ☐ ☐ 4 Sense of responsibility Completed assigned modules on time and ensured code quality ✅ ☐ ☐ 5 Discipline Followed Git workflow, meetings, and development processes ☐ ☐ ✅ 6 Progressive mindset Actively accepted feedback and continuously improved architecture and coding style ☐ ✅ ☐ 7 Communication Communicated technical ideas clearly during discussions and documentation ☐ ✅ ☐ 8 Teamwork Collaborated effectively with teammates on architecture and development tasks ✅ ☐ ☐ 9 Professional conduct Showed respect, responsibility, and positivity in the work environment ✅ ☐ ☐ 10 Problem-solving skills Troubleshot CI/CD errors, caching issues, VPC networking, and deployment problems ☐ ✅ ☐ 11 Contribution to project/team Built major components of the e-commerce platform and AWS architecture ✅ ☐ ☐ 12 Overall Overall performance throughout the internship ✅ ☐ ☐ Needs Improvement Improve workplace discipline such as consistent documentation habits, task reporting, and time management Strengthen system debugging and problem-solving techniques, especially for distributed cloud environments Enhance communication skills in technical presentation and teamwork discussions Continue learning advanced AWS services, security best practices, and performance optimization "
},
{
	"uri": "http://localhost:1313/AWS-fcj/5-workshop/5.6-cicd/",
	"title": "CI/CD Automation",
	"tags": [],
	"description": "",
	"content": "Overview In the Cloud Native environment, manual deployment (Manual Deployment) is risky and time-consuming. This module will guide you to build a fully automated CI/CD (Continuous Integration / Continuous Deployment) process\nThe process operates as follows:\nSource: Developer pushes code (Push) to GitHub Build: AWS CodePipeline detects changes and activates AWS CodeBuild. CodeBuild will package the Docker Image and push to the Amazon ECR repository Deploy: Pipeline automatically commands Elastic Beanstalk to update the latest version from ECR without service interruption Content Create Build Project with AWS CodeBuild Set up AWS CodePipeline "
},
{
	"uri": "http://localhost:1313/AWS-fcj/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives Explore and practice AWS Data \u0026amp; Analytics services. Learn and experiment with Machine Learning (ML) services such as SageMaker and Rekognition. Strengthen skills in monitoring, auditing, and system performance optimization using CloudWatch, CloudTrail, and Trusted Advisor. Study Advanced Identity Management in AWS, including IAM Roles, Federated Access, and AWS SSO. Time Period: 03/11/2025 - 09/11/2025\nWeekly Task Overview Day Activity Start Date End Date Reference 1 - Study Databases in AWS + Compare relational vs. NoSQL databases + Create test databases using RDS and DynamoDB + Explore Redshift and ElastiCache for query acceleration 2025-11-03 2025-11-03 https://aws.amazon.com/databases/ 2 - Research Advanced Identity in AWS + Create and manage advanced IAM Roles + Learn about Federated Access + Configure AWS SSO and test multi-account access 2025-11-04 2025-11-04 https://aws.amazon.com/iam/ 3 - Analyze requirements for the Mini-Market System Project + Identify core modules: Products, Inventory, Billing–Payment, Users + Build Use Cases, Entity Diagrams, and business requirement documentation 2025-11-05 2025-11-05 - 4 - Design the Solution Architecture for the system + Choose .NET 3-tier Architecture + Apply Repository Pattern \u0026amp; Unit of Work Pattern + Design backend diagram, database structure, and user permission model 2025-11-06 2025-11-06 https://aws.amazon.com/architecture/ 5 - Summaries and consolidate weekly knowledge + Review advanced architecture, IoT, and AWS Whitepapers + Compare Hybrid, Multi-Cloud, and Serverless architectures 2025-11-07 2025-11-07 https://aws.amazon.com/cloudwatch/ 6 - Learn more about Advanced Identity in AWS + Create and manage advanced IAM Roles + Begin preparing content for the AWS Cloud Journey Final Report 2025-11-08 2025-11-08 https://aws.amazon.com/iam/ Week 6 Achievements Mastered core concepts of AWS Data \u0026amp; Analytics:\nUnderstood data analysis workflows using Athena, Glue, EMR, and Kinesis. Queried S3 data with Athena and built a basic ETL pipeline using Glue. Created a real-time data streaming pipeline using Kinesis Data Stream. Progressed in Machine Learning on AWS:\nTrained and deployed a sample model using SageMaker. Used Amazon Rekognition for face and object detection in images. Explored additional ML services such as Comprehend, Translate, and Textract. Developed strong skills in system monitoring and auditing:\nMonitored metrics and logs with CloudWatch. Tracked API activities using CloudTrail. Reviewed recommendations from Trusted Advisor to improve performance and reduce cost. Gained deeper understanding of Advanced Identity Management:\nCreated and applied IAM Roles following the least privilege principle. Learned how Federated Access allows login via organizational identity providers. Configured AWS SSO for secure multi-account access. Strengthened the integration of Data, Machine Learning, Monitoring, and Security concepts, preparing for next week’s topics on Automation \u0026amp; Infrastructure as Code (IaC).\n"
},
{
	"uri": "http://localhost:1313/AWS-fcj/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nHere, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment was supportive, comfortable, and easy to adapt to. Everyone was willing to help whenever I encountered technical issues—especially with AWS setup, Elastic Beanstalk deployment, or CI/CD troubleshooting. The flexible and remote-friendly environment fit well with my schedule. I think the program could add more team-sharing sessions or bonding activities to strengthen connections among interns.\n2. Support from Mentor / Team Admin\nThe mentor provided detailed and clear guidance, especially when dealing with complex topics such as AWS architecture design, VPC networking, CI/CD pipeline setup, and .NET backend optimization. Instead of giving direct answers, the mentor encouraged me to analyze and solve problems independently, which I highly appreciate. The admin team was also very helpful with documents, logistics, and general support.\n3. Relevance of Work to Academic Major\nThe tasks assigned to me matched well with the knowledge I learned in school while also extending to new areas that I had never worked with before. Working with .NET 3-tier architecture, EF Core, repository pattern, SQL Server, and AWS services helped reinforce both theoretical and practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nThe internship allowed me to gain valuable hands-on skills, including:\nAWS cloud architecture (VPC, RDS, S3, CloudFront, WAF, NAT Gateway, Elastic Beanstalk, etc.) Building CI/CD pipelines using CodePipeline \u0026amp; CodeBuild Designing systems following AWS Well-Architected Framework Writing technical documentation and analyzing software architecture Soft skills such as teamwork, progress reporting, communication, and handling issues professionally also improved significantly throughout the program.\n5. Company Culture \u0026amp; Team Spirit\nThe team culture was positive and respectful. Everyone worked seriously but was still friendly and supportive. Even during busy periods, team members helped each other without hesitation. This made me feel like I was part of the team, not just an intern.\n6. Internship Policies / Benefits\nThe program provided flexibility in working hours and gave access to useful learning materials. The opportunity to work on an end-to-end real project was one of the biggest benefits, helping me build practical experience beyond what is taught academically.\nAdditional Questions What was I most satisfied with during the internship?\nThe opportunity to work on a complete cloud-based system — designing architecture, deploying services, testing, and optimizing costs — was incredibly valuable.\nWhat should the program improve for future interns?\nIt would be beneficial to include more focused technical workshops (e.g., deeper VPC networking, advanced AWS configurations) or scheduled study sessions.\nWould I recommend this internship to others? Why?\nYes. Because the program offers real project experience, strong mentorship, and practical skill development that directly benefit career growth.\nSuggestions \u0026amp; Expectations Consider adding more AWS-focused workshops or hands-on labs. Introduce periodic demo sessions or sprint reviews so interns can learn from each other’s progress. Provide additional documentation or standardized AWS deployment checklists. If an advanced or extended internship track becomes available, I would be excited to continue participating. "
},
{
	"uri": "http://localhost:1313/AWS-fcj/5-workshop/5.7-security/",
	"title": "Optimization &amp; Security",
	"tags": [],
	"description": "",
	"content": "Resource Cleanup Overview A Production system needs not only to \u0026ldquo;run\u0026rdquo; but also to \u0026ldquo;run fast\u0026rdquo; and be \u0026ldquo;secure\u0026rdquo;. In this part, we will refine the MiniMarket architecture\nImplementation items:\nOffloading Static Assets: Transfer all product images from Web Server to Amazon S3 and distribute via Amazon CloudFront (CDN) to accelerate global page load speed and offload the server Security Hardening: Deploy AWS WAF (Web Application Firewall) in front of CloudFront to protect the application from common attacks such as SQL Injection and XSS Content Configure S3 and CloudFront Set up AWS WAF "
},
{
	"uri": "http://localhost:1313/AWS-fcj/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives Understand and apply data and system security practices in AWS. Configure, manage, and optimize Amazon VPC (Virtual Private Cloud) for secure private networking. Learn how to back up, restore, and implement Disaster Recovery (DR) using AWS services. Time Period: 10/11/2025 - 16/11/2025\nWeekly Task Overview Day Activity Start Date End Date Reference 1 - Begin writing the project Proposal + Complete the Executive Summary, Problem Statement, and Overall System Architecture + Research AWS services to be integrated in later deployment phases 2025-11-10 2025-11-10 - 2 - Build Domain → Persistence → Application Layer for the Mini-Market system + Create entities + Implement Repository \u0026amp; Unit of Work Pattern + Run migrations \u0026amp; seed data + Develop core business logic services 2025-11-11 2025-11-11 - 3 - Initialize the WebShop MVC project + Create base layout (Header, Footer, Navigation) + Build Product List – Shopping Cart pages + Write the Architecture \u0026amp; Technical Implementation section of the Proposal 2025-11-12 2025-11-12 - 4 - Draw the AWS Architecture Diagram for the system + Collect information for each service used: · Elastic Beanstalk · RDS SQL Server · Amazon S3 · CloudFront · WAF \u0026amp; Route 53 · ElastiCache Redis · NAT Gateway · CodePipeline/CodeBuild 2025-11-13 2025-11-13 - 5 - Study Disaster Recovery \u0026amp; Backup + Use AWS Backup for automated resource backup + Configure RDS Read Replicas for improved fault tolerance + Practice Cross-Region Failover for DR planning 2025-11-14 2025-11-14 https://aws.amazon.com/backup/ 6 - Summarize project progress and update the Proposal + Review architecture, IAM roles, and selected AWS services + Prepare content for Week 8: S3 upload integration, Redis caching, and architecture optimization 2025-11-15 2025-11-15 - Week 7 Achievements Strengthened knowledge of AWS security and data encryption:\nPracticed creating and managing KMS keys for data protection. Understood how CloudHSM enhances hardware-level key security. Configured AWS Shield and WAF to safeguard applications from DDoS attacks and malicious traffic. Gained solid understanding of Amazon VPC networking:\nCreated and configured VPC, subnets, route tables, Internet Gateway, and NAT Gateway. Applied Security Groups and Network ACLs for effective access control. Set up VPN Connection and studied Direct Connect for secure, high-throughput connectivity. Mastered core concepts of Disaster Recovery (DR) and data backup:\nUsed AWS Backup for automated scheduled backups. Configured RDS Read Replicas to ensure data availability. Practiced Cross-Region Failover to maintain system continuity during outages. Improved ability to design secure, reliable, and highly resilient AWS architectures:\nLearned to integrate Security, Networking, and Resilience best practices. Prepared foundational knowledge for next week’s topics: Automation, Infrastructure as Code (IaC), and DevOps Practices. "
},
{
	"uri": "http://localhost:1313/AWS-fcj/5-workshop/5.8-monitoring/",
	"title": "Monitoring &amp; Operations",
	"tags": [],
	"description": "",
	"content": "Overview A Production system cannot be considered complete without Monitoring and Alerting capabilities. You cannot sit watching the screen 24/7 to check if the Server is still alive\nIn this module, we will set up monitoring and alerting for MiniMarket using AWS operations management services:\nAmazon CloudWatch: Collect metrics (Metrics) from EC2, RDS, ELB Amazon SNS (Simple Notification Service): Notification service. We will use it to send Emails to administrators when the system encounters issues We will set up a CloudWatch Alarm to monitor Web Server CPU. If CPU exceeds 70% (sign of overload or attack), the system will automatically trigger SNS to send emergency alert Emails\nContent Set up CloudWatch Alarms \u0026amp; SNS "
},
{
	"uri": "http://localhost:1313/AWS-fcj/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives Strengthen understanding of Solutions Architecture in AWS environments. Explore additional extended AWS services such as IoT, Ground Station, and RoboMaker. Study in-depth whitepapers, reference architectures, and best practices provided by AWS. Time Period: 17/11/2025 - 22/11/2025\nWeekly Task Overview Day Activity Start Date End Date Reference 1 - Integrate Amazon S3 \u0026amp; ElastiCache (Redis) + Implement product image upload to S3 + Configure Redis caching to improve product list performance 2025-11-17 2025-11-17 https://aws.amazon.com/s3/ 2 - Draft the overall system architecture + Review AWS sample architectures (3-tier, e-commerce) + Identify services to be used: EC2/Elastic Beanstalk, RDS, S3, CloudFront, VPC, etc. 2025-11-18 2025-11-18 https://aws.amazon.com/architecture/ 3 - Optimize data model \u0026amp; service layer for the project + Review entities and mapping + Adjust business logic flow to match AWS deployment requirements 2025-11-19 2025-11-19 - 4 - Complete the Proposal Document + Outline Executive Summary, Problem Statement, Solution Architecture, Implementation Plan, Cost Estimation + Prepare detailed content for the upcoming weeks 2025-11-19 2025-11-19 - 5 - Review architecture and finalize Proposal + Verify compatibility between selected AWS services + Adjust solution design to ensure scalability, high availability, and cost optimization 2025-11-20 2025-11-20 https://aws.amazon.com/whitepapers/ 6 - Week 8 Summary + Summarize concepts in architecture, IoT services, and whitepapers + Compare hybrid, multi-cloud, and serverless architectures + Prepare content for the AWS Cloud Journey final report 2025-11-20 2025-11-20 - Week 8 Achievements Strengthened understanding of Advanced Solutions Architecture:\nDesigned hybrid architectures combining on-premise infrastructure and AWS Cloud. Understood Multi-Cloud models and appropriate use cases. Explored architectural Design Patterns such as Microservices, Event-Driven, and Serverless. Hands-on experience with extended AWS services:\nConnected IoT devices using AWS IoT Core. Learned data acquisition and satellite processing workflows via AWS Ground Station. Explored robotic simulation and automation using AWS RoboMaker. In-depth review of AWS Whitepapers:\nStudied the AWS Well-Architected Framework and its 5 pillars:\nOperational Excellence, Security, Reliability, Performance Efficiency, Cost Optimization. Learned strategies for High Availability and Disaster Recovery. Collected best practices on security, cost optimization, and scalable architectures. Week 8 consolidation:\nCombined architectural theory with hands-on AWS services. Prepared content for the final project presentation and complete system architecture documentation. "
},
{
	"uri": "http://localhost:1313/AWS-fcj/5-workshop/5.9-cleanup/",
	"title": "Resource Cleanup",
	"tags": [],
	"description": "",
	"content": "Overview Congratulations on successfully deploying MiniMarket on AWS!\nHowever, our work does not end there. The final step and also the most important step to protect your \u0026ldquo;wallet\u0026rdquo; is Resource Cleanup\nThe services we deployed such as NAT Gateway, Elastic Load Balancer, RDS, ElastiCache are all billed by the hour, whether you use them or not. If you forget to delete, the month-end bill can be very high\nWe will go through the system Decommissioning process in the correct order to ensure no resources are left behind causing hidden costs\nContent Safe resource deletion process "
},
{
	"uri": "http://localhost:1313/AWS-fcj/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Goals Complete the final AWS Cloud Journey summary report. Build and present a full AWS system architecture based on all services learned. Review and consolidate knowledge from Week 1 to Week 8. Prepare materials for the final presentation. Weekly Tasks Overview Day Activity Start Date End Date Reference 1 - Consolidated learning from Week 1–8 + Reviewed all major AWS services + Assessed understanding of Compute, Storage, Networking, and Security 11/03/2025 11/03/2025 - 2 - Built the final architecture diagram + Designed full VPC layout + Included Compute, RDS, S3, Load Balancer, IAM, and Monitoring 11/04/2025 11/04/2025 https://aws.amazon.com/architecture/icons/ 3 - Wrote the final Well-Architected report + Security pillar + Reliability pillar + Performance efficiency + Cost optimization + Operational excellence 11/05/2025 11/05/2025 https://aws.amazon.com/architecture/well-architected/ 4 - Reviewed key advanced topics + Advanced VPC networking, VPN, Direct Connect + Auto Scaling + Load Balancing + S3 security \u0026amp; CloudFront 11/06/2025 11/06/2025 - 5 - Prepared Final Presentation materials + Architecture slides + Demo outline (if applicable) 11/07/2025 11/07/2025 - 6 - Final weekly summary + Updated worklog and Hugo site + Performed final content review before submission 11/08/2025 11/08/2025 - Week 9 Achievements Completed the full AWS system architecture design, including:\nVPC with public/private subnets EC2 + ALB + Auto Scaling RDS + S3 + CloudFront IAM roles and policies Security Groups, NACLs CloudWatch \u0026amp; CloudTrail monitoring Delivered a complete AWS Cloud Journey Final Report, summarizing:\nAll concepts learned Hands-on labs completed Final architecture and design rationale Strengthened core AWS knowledge:\nCompute, Storage, Networking, Security Serverless technologies Monitoring and cost optimization Prepared full presentation materials, diagrams, and summary slides.\nFully ready for the program’s Final Presentation Session.\n"
},
{
	"uri": "http://localhost:1313/AWS-fcj/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Goals Complete the full Project Proposal required for the program. Study core AWS architectural concepts to accurately design and draw an AWS Architecture Diagram. Prepare system flows, service selection, and architectural components for the final project. Weekly Tasks Overview Day Activity Start Date End Date Reference 1 - Started writing the Project Proposal + Drafted Executive Summary + Defined the Problem Statement and project objectives 11/10/2025 11/10/2025 - 2 - Completed the Solution Overview section + Listed required AWS services + Defined the primary system data flow 11/11/2025 11/11/2025 https://aws.amazon.com/architecture/ 3 - Studied AWS architectural design principles + Reviewed 3-tier, Serverless, and Event-driven architectures + Learned AWS Architecture Icons 11/12/2025 11/12/2025 https://aws.amazon.com/architecture/icons/ 4 - Practiced drawing the AWS Architecture Diagram + Designed VPC and subnet layout + Added ALB, EC2, RDS, S3, IAM, and routing components + Mapped security layers and connectivity 11/13/2025 11/13/2025 - 5 - Updated the Proposal based on the drafted architecture + Inserted the Architecture Diagram + Wrote the Technical Implementation section 11/14/2025 11/14/2025 - 6 - Final Week 10 review + Fully reviewed the completed Proposal + Performed final architecture validation 11/15/2025 11/15/2025 - Week 10 Achievements Completed the full Project Proposal, including:\nExecutive Summary Problem Statement Solution Architecture Expected Outcomes Technical Implementation Risk Assessment Gained strong understanding of AWS Architecture Design:\nAWS Architecture Icons and diagram conventions VPC + Subnet design principles Compute, Database, Storage, and Networking placement in architecture diagrams Produced the initial Architecture Diagram, containing:\nVPC with public and private subnets ALB + EC2 Auto Scaling Group RDS Database S3 storage layer IAM roles, Security Groups, and network controls Successfully prepared all foundational components needed for the upcoming final presentation and project submission.\n"
},
{
	"uri": "http://localhost:1313/AWS-fcj/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 11 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 11 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/AWS-fcj/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 12 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 12 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/AWS-fcj/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/AWS-fcj/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]